# -*- coding: utf-8 -*-
"""customedPipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11o3lLBWeIGO4_yRU4NIre7XC1ps4xaDM
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import Phi3Config
messages = [[
    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"},
    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."},
    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"},
],
[{"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"}]]

import gc
gc.collect()

torch.random.manual_seed(0)
model_id = "microsoft/Phi-3-medium-4k-instruct"
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     device_map="cuda",
#     torch_dtype="auto",
#     trust_remote_code=True,
# )


class CustomedPipeline():
    def __init__(
            self,
            model,
            config,
            model_id = "microsoft/Phi-3-medium-4k-instruct",
            device = "cuda"
        ):
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model =  model

    def preprocess(
            self,
            prompt_text,
            prefix="",
            handle_long_generation=None,
            add_special_tokens=None
            ):

        inputs = self.tokenizer.apply_chat_template(
                prompt_text,
                add_generation_prompt=True,
                tokenize=True,
                return_tensors="pt",
            return_dict=True,
                padding=True
            ).to('cuda')
        inputs['prompts'] = inputs['input_ids'].shape[-1]

        return inputs

    def forward(self, model_inputs, max_length = 500):
        input_ids = model_inputs['input_ids']
        attention_mask = model_inputs['attention_mask']
        prompt_len = model_inputs['prompts']

        generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,max_length=max_length)
        return {"generated_sequence": generated_sequence, "prompt_len" :prompt_len}

    def postprocess(self, model_outputs, clean_up_tokenization_spaces=True):
        generated_sequence = model_outputs["generated_sequence"]
        prompt_len = model_outputs["prompt_len"]

        result = []

        for i, text in enumerate(generated_sequence):
            eos_pos = (text == self.tokenizer.eos_token_id).nonzero(as_tuple=True)[0]

            if len(eos_pos) > 0:
                eos_after_prompt = next((pos.item() for pos in eos_pos if pos.item() > prompt_len), None)

                if eos_after_prompt is not None:
                    text = text[prompt_len:eos_after_prompt-1]
                else:
                    text = text[prompt_len:]
            else:
                text = text[prompt_len:]

            #decoded_text = self.tokenizer.decode(text, skip_special_tokens=True)
            decoded_text = self.tokenizer.decode(text)
            result.append([{'generated':decoded_text}])

        return result


