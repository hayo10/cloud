# -*- coding: utf-8 -*-
"""customedPipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11o3lLBWeIGO4_yRU4NIre7XC1ps4xaDM
"""

import torch
from transformers import AutoTokenizer
from transformers import Phi3Config
import time
import gc
gc.collect()

torch.random.manual_seed(0)
model_id = "microsoft/Phi-3-medium-4k-instruct"
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     device_map="cuda",
#     torch_dtype="auto",
#     trust_remote_code=True,
# )


class CustomedPipeline():
    def __init__(
            self,
            model,
            config,
            model_id = "microsoft/Phi-3-medium-4k-instruct",
            device = "cuda"
        ):
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model =  model



    def forward(self, input_ids, attention_mask):
        times = 0
        result = []
        
        for batch in zip(input_ids, attention_mask):
            inputs = batch[0].to('cuda')
            masks = batch[1].to('cuda')
            #prompt_len = batch[0].shape[1]
        
            st = time.time()

            generated_sequence = self.model.generate(input_ids=inputs, attention_mask=masks, max_new_tokens=10 )
            end = time.time()
            print('batch inference time ', end - st)
            result.append(generated_sequence)
            times += end - st
        return {"generated_sequence": result}

    def find_pattern(self, text):
        idx = []
        for i in range(91,len(text)-1):
            if text[i] == 32007 and text[i+1]==32001:
                idx.append(i)
           
        if len(idx) == 0:
            return text[90:]
        elif len(idx) == 1:
            return text[idx[0]+2:]
        else:
            return text[idx[0]+2: idx[1]]

    def postprocess(self,model_outputs, labels, clean_up_tokenization_spaces=True):

        result = []

        correct = 0
        
        for outputs in model_outputs['generated_sequence']:
            for i, text in enumerate(outputs):
                answer = self.find_pattern(text)
                decoded_answer = self.tokenizer.decode(answer)
                if decoded_answer == labels[i]:
                    correct += 1
                result.append([{'generated':decoded_answer, 'label' : labels[i]}])
            

        print('accuracy : ',correct/len(model_outputs)*len(outputs))
        return result


